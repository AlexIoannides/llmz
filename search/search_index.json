{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLMz","text":"<p>These will be the docs for the llmz project.</p>"},{"location":"api/","title":"API Reference","text":"<p>Attention blocks for transformer models.</p> <p>Normalisation operations.</p> <p>Activation functions for transformer models.</p> <p>Transformer block for LLMs.</p> <p>Datasets for LLMs.</p> <p>Evaluation and metrics.</p> <p>Tools for text generation .</p> <p>Implementation of GPT2.</p> <p>Functions for training LLMs.</p>"},{"location":"api/#llmz.components.attention.ModelConfigError","title":"<code>ModelConfigError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception class for model configuration errors.</p> Source code in <code>src/llmz/components/attention.py</code> <pre><code>class ModelConfigError(Exception):\n    \"\"\"Custom exception class for model configuration errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#llmz.components.attention.MultiHeadAttention","title":"<code>MultiHeadAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Basic causal attention block.</p> Source code in <code>src/llmz/components/attention.py</code> <pre><code>class MultiHeadAttention(nn.Module):\n    \"\"\"Basic causal attention block.\"\"\"\n\n    def __init__(\n        self,\n        context_size: int,\n        dim_in: int,\n        dim_out: int,\n        n_heads: int = 1,\n        dropout: float = 0.6,\n        qkv_bias: bool = False,\n    ):\n        \"\"\"Initialise module.\n\n        Args:\n            dim_in: Dimension of input word embeddings.\n            dim_out: Dimension of output attention embeddings.\n            context_size: The number of input word embeddings in the sequence.\n            n_heads: The number of attention heads. Defaults to 1.\n            dropout: The dropout rate. Defaults to 0.6.\n            qkv_bias: Whether or not to include bias in the linear layers used to\n                compute W_query, W_key and W_value. Defaults to False.\n\n        Raises:\n            ModelConfigError: if dim_out % n_heads\n\n        \"\"\"\n        super().__init__()\n        if dim_out % n_heads != 0:\n            raise ModelConfigError(\"dim_out % n_heads != 0\")\n\n        self.dim_out = dim_out\n        self.n_heads = n_heads\n        self.dim_head = dim_out // n_heads  # // --&gt; returns int\n        self.W_query = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n        self.W_key = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n        self.W_value = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(dim_out, dim_out)\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            \"mask\", torch.triu(torch.ones(context_size, context_size), diagonal=1)\n        )  # these are not parameters\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Execute the module's forward pass.\n\n        Args:\n            x: Batch of token embeddings.\n\n        Returns:\n            Batch of attention weighted embeddings.\n\n        \"\"\"\n        batch_size, seq_len, dim_in = x.size()\n\n        # get mask for sequence length\n        mask_bool = self.mask.bool()[:seq_len, :seq_len]\n\n        # single head (dim = batch_size, n_heads, seq_len, dim_out)\n        queries = self.W_query(x)\n        keys = self.W_key(x)\n        values = self.W_value(x)\n\n        # split single head into multiple heads\n        queries = queries.view(batch_size, seq_len, self.n_heads, self.dim_head)\n        keys = keys.view(batch_size, seq_len, self.n_heads, self.dim_head)\n        values = values.view(batch_size, seq_len, self.n_heads, self.dim_head)\n\n        # reshape in size = batch_size, n_heads, seq_len, head_dim\n        queries = queries.transpose(1, 2)\n        keys = keys.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # compute attention scores (matrix multiplication works on final two dimensions)\n        attn_scores = queries @ keys.transpose(2, 3)\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        # compute attention weights from attention scores (dim = -1 -&gt; last dim in size)\n        attn_weights = torch.softmax(attn_scores / keys.size()[-1] ** 0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # compute context embeddings &amp; reshape to batch_size, seq_len, n_heads, head_dim\n        context_embeddings = (attn_weights @ values).transpose(1, 2)\n\n        # reshape to factor-out the multiple heads and take into account dim_out\n        context_embeddings = context_embeddings.view(batch_size, seq_len, self.dim_out)\n        context_embeddings = self.out_proj(context_embeddings)\n        return context_embeddings\n</code></pre>"},{"location":"api/#llmz.components.attention.MultiHeadAttention.__init__","title":"<code>__init__(context_size, dim_in, dim_out, n_heads=1, dropout=0.6, qkv_bias=False)</code>","text":"<p>Initialise module.</p> <p>Parameters:</p> Name Type Description Default <code>dim_in</code> <code>int</code> <p>Dimension of input word embeddings.</p> required <code>dim_out</code> <code>int</code> <p>Dimension of output attention embeddings.</p> required <code>context_size</code> <code>int</code> <p>The number of input word embeddings in the sequence.</p> required <code>n_heads</code> <code>int</code> <p>The number of attention heads. Defaults to 1.</p> <code>1</code> <code>dropout</code> <code>float</code> <p>The dropout rate. Defaults to 0.6.</p> <code>0.6</code> <code>qkv_bias</code> <code>bool</code> <p>Whether or not to include bias in the linear layers used to compute W_query, W_key and W_value. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ModelConfigError</code> <p>if dim_out % n_heads</p> Source code in <code>src/llmz/components/attention.py</code> <pre><code>def __init__(\n    self,\n    context_size: int,\n    dim_in: int,\n    dim_out: int,\n    n_heads: int = 1,\n    dropout: float = 0.6,\n    qkv_bias: bool = False,\n):\n    \"\"\"Initialise module.\n\n    Args:\n        dim_in: Dimension of input word embeddings.\n        dim_out: Dimension of output attention embeddings.\n        context_size: The number of input word embeddings in the sequence.\n        n_heads: The number of attention heads. Defaults to 1.\n        dropout: The dropout rate. Defaults to 0.6.\n        qkv_bias: Whether or not to include bias in the linear layers used to\n            compute W_query, W_key and W_value. Defaults to False.\n\n    Raises:\n        ModelConfigError: if dim_out % n_heads\n\n    \"\"\"\n    super().__init__()\n    if dim_out % n_heads != 0:\n        raise ModelConfigError(\"dim_out % n_heads != 0\")\n\n    self.dim_out = dim_out\n    self.n_heads = n_heads\n    self.dim_head = dim_out // n_heads  # // --&gt; returns int\n    self.W_query = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n    self.W_key = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n    self.W_value = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n    self.out_proj = nn.Linear(dim_out, dim_out)\n    self.dropout = nn.Dropout(dropout)\n    self.register_buffer(\n        \"mask\", torch.triu(torch.ones(context_size, context_size), diagonal=1)\n    )  # these are not parameters\n</code></pre>"},{"location":"api/#llmz.components.attention.MultiHeadAttention.forward","title":"<code>forward(x)</code>","text":"<p>Execute the module's forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Batch of token embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Batch of attention weighted embeddings.</p> Source code in <code>src/llmz/components/attention.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Execute the module's forward pass.\n\n    Args:\n        x: Batch of token embeddings.\n\n    Returns:\n        Batch of attention weighted embeddings.\n\n    \"\"\"\n    batch_size, seq_len, dim_in = x.size()\n\n    # get mask for sequence length\n    mask_bool = self.mask.bool()[:seq_len, :seq_len]\n\n    # single head (dim = batch_size, n_heads, seq_len, dim_out)\n    queries = self.W_query(x)\n    keys = self.W_key(x)\n    values = self.W_value(x)\n\n    # split single head into multiple heads\n    queries = queries.view(batch_size, seq_len, self.n_heads, self.dim_head)\n    keys = keys.view(batch_size, seq_len, self.n_heads, self.dim_head)\n    values = values.view(batch_size, seq_len, self.n_heads, self.dim_head)\n\n    # reshape in size = batch_size, n_heads, seq_len, head_dim\n    queries = queries.transpose(1, 2)\n    keys = keys.transpose(1, 2)\n    values = values.transpose(1, 2)\n\n    # compute attention scores (matrix multiplication works on final two dimensions)\n    attn_scores = queries @ keys.transpose(2, 3)\n    attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n    # compute attention weights from attention scores (dim = -1 -&gt; last dim in size)\n    attn_weights = torch.softmax(attn_scores / keys.size()[-1] ** 0.5, dim=-1)\n    attn_weights = self.dropout(attn_weights)\n\n    # compute context embeddings &amp; reshape to batch_size, seq_len, n_heads, head_dim\n    context_embeddings = (attn_weights @ values).transpose(1, 2)\n\n    # reshape to factor-out the multiple heads and take into account dim_out\n    context_embeddings = context_embeddings.view(batch_size, seq_len, self.dim_out)\n    context_embeddings = self.out_proj(context_embeddings)\n    return context_embeddings\n</code></pre>"},{"location":"api/#llmz.components.normalisation.LayerNormalisation","title":"<code>LayerNormalisation</code>","text":"<p>               Bases: <code>Module</code></p> <p>Layer normalisation.</p> <p>Normalises batches of input tensors close zero mean and unit variance. The module allows for some trained deviation from the a mean of zero and a variance of one.</p> Source code in <code>src/llmz/components/normalisation.py</code> <pre><code>class LayerNormalisation(nn.Module):\n    \"\"\"Layer normalisation.\n\n    Normalises batches of input tensors close zero mean and unit variance. The module\n    allows for some trained deviation from the a mean of zero and a variance of one.\n    \"\"\"\n\n    def __init__(self, dim_in: int):\n        \"\"\"Initialise module.\n\n        Args:\n            dim_in: Dimension of the input batches.\n\n        \"\"\"\n        super().__init__()\n        self.epsilon = 1e-5\n\n        # Trainable element-by-element adjustments to output tensors\n        self.shift = nn.Parameter(torch.zeros(dim_in))\n        self.scale = nn.Parameter(torch.ones(dim_in))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of module.\n\n        Args:\n            x: input tensors.\n\n        Returns:\n            Tensor-by-tensor normalised version of the inputs.\n\n        \"\"\"\n        x_mean = x.mean(dim=-1, keepdim=True)\n        x_stdev = x.std(dim=-1, keepdim=True, unbiased=False)  # unbiased as n -&gt; inf\n        x_norm = (x - x_mean) / (x_stdev + self.epsilon)\n        return self.shift + self.scale * x_norm\n</code></pre>"},{"location":"api/#llmz.components.normalisation.LayerNormalisation.__init__","title":"<code>__init__(dim_in)</code>","text":"<p>Initialise module.</p> <p>Parameters:</p> Name Type Description Default <code>dim_in</code> <code>int</code> <p>Dimension of the input batches.</p> required Source code in <code>src/llmz/components/normalisation.py</code> <pre><code>def __init__(self, dim_in: int):\n    \"\"\"Initialise module.\n\n    Args:\n        dim_in: Dimension of the input batches.\n\n    \"\"\"\n    super().__init__()\n    self.epsilon = 1e-5\n\n    # Trainable element-by-element adjustments to output tensors\n    self.shift = nn.Parameter(torch.zeros(dim_in))\n    self.scale = nn.Parameter(torch.ones(dim_in))\n</code></pre>"},{"location":"api/#llmz.components.normalisation.LayerNormalisation.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensors.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor-by-tensor normalised version of the inputs.</p> Source code in <code>src/llmz/components/normalisation.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of module.\n\n    Args:\n        x: input tensors.\n\n    Returns:\n        Tensor-by-tensor normalised version of the inputs.\n\n    \"\"\"\n    x_mean = x.mean(dim=-1, keepdim=True)\n    x_stdev = x.std(dim=-1, keepdim=True, unbiased=False)  # unbiased as n -&gt; inf\n    x_norm = (x - x_mean) / (x_stdev + self.epsilon)\n    return self.shift + self.scale * x_norm\n</code></pre>"},{"location":"api/#llmz.components.activations.GELU","title":"<code>GELU</code>","text":"<p>               Bases: <code>Module</code></p> <p>Guassian Error Linear Unit (GELU).</p> <p>Implemented using an approximation to <code>x * F(x)</code>, where <code>F</code> is the cumulative normal distribution function. See 'Build a LLM (from scratch)' by S. Raschka (2024), p105.</p> Source code in <code>src/llmz/components/activations.py</code> <pre><code>class GELU(nn.Module):\n    \"\"\"Guassian Error Linear Unit (GELU).\n\n    Implemented using an approximation to `x * F(x)`, where `F` is the cumulative\n    normal distribution function. See 'Build a LLM (from scratch)' by S. Raschka\n    (2024), p105.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialise module.\"\"\"\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Execute the module's forward pass.\n\n        Args:\n            x: Batch of input tensors.\n\n        Returns:\n            Batch of output tensors that have been filtered on an element-by-element\n                basis using the GELU activation function.\n\n        \"\"\"\n        tanh_exponent = torch.sqrt(torch.tensor(2.0 / torch.pi)) * (\n            x + 0.044715 * torch.pow(x, 3)\n        )\n        gelu_x = 0.5 * x * (1.0 + torch.tanh(tanh_exponent))\n        return gelu_x\n</code></pre>"},{"location":"api/#llmz.components.activations.GELU.__init__","title":"<code>__init__()</code>","text":"<p>Initialise module.</p> Source code in <code>src/llmz/components/activations.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialise module.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/#llmz.components.activations.GELU.forward","title":"<code>forward(x)</code>","text":"<p>Execute the module's forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Batch of input tensors.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Batch of output tensors that have been filtered on an element-by-element basis using the GELU activation function.</p> Source code in <code>src/llmz/components/activations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Execute the module's forward pass.\n\n    Args:\n        x: Batch of input tensors.\n\n    Returns:\n        Batch of output tensors that have been filtered on an element-by-element\n            basis using the GELU activation function.\n\n    \"\"\"\n    tanh_exponent = torch.sqrt(torch.tensor(2.0 / torch.pi)) * (\n        x + 0.044715 * torch.pow(x, 3)\n    )\n    gelu_x = 0.5 * x * (1.0 + torch.tanh(tanh_exponent))\n    return gelu_x\n</code></pre>"},{"location":"api/#llmz.components.transformers.TransformerBlockGPT2","title":"<code>TransformerBlockGPT2</code>","text":"<p>               Bases: <code>Module</code></p> <p>Basic transformer block with multi-head attention as used in GPT2.</p> Source code in <code>src/llmz/components/transformers.py</code> <pre><code>class TransformerBlockGPT2(nn.Module):\n    \"\"\"Basic transformer block with multi-head attention as used in GPT2.\"\"\"\n\n    def __init__(\n        self,\n        context_size: int,\n        dim_in: int,\n        n_heads: int = 1,\n        dropout: float = 0.6,\n        qkv_bias: bool = False,\n    ):\n        \"\"\"Initialise module.\n\n        Args:\n            dim_in: Dimension of input word embeddings.\n            context_size: The number of input word embeddings in the sequence.\n            n_heads: The number of attention heads. Defaults to 1.\n            dropout: The dropout rate. Defaults to 0.6.\n            qkv_bias: Whether or not to include bias in the linear layers used to\n                compute W_query, W_key and W_value. Defaults to False.\n\n        \"\"\"\n        super().__init__()\n        self.attention = MultiHeadAttention(\n            context_size, dim_in, dim_in, n_heads, dropout, qkv_bias\n        )\n        self.linear_1 = nn.Linear(dim_in, dim_in * 2)\n        self.linear_2 = nn.Linear(dim_in * 2, dim_in)\n        self.normalise_1 = LayerNormalisation(dim_in)\n        self.normalise_2 = LayerNormalisation(dim_in)\n        self.dropout = nn.Dropout(dropout)\n        self.gelu = GELU()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Execute the module's forward pass.\n\n        Args:\n            x: Batch of token embeddings.\n\n        Returns:\n            Batch of attention weighted embeddings.\n\n        \"\"\"\n        y1 = self.normalise_1(x)\n        y1 = self.attention(y1)\n        y1 = self.dropout(y1)\n\n        y2 = self.normalise_2(y1 + x)\n        y2 = self.linear_1(y2)\n        y2 = self.gelu(y2)\n        y2 = self.linear_2(y2)\n        y2 = self.dropout(y2)\n\n        return y1 + y2\n</code></pre>"},{"location":"api/#llmz.components.transformers.TransformerBlockGPT2.__init__","title":"<code>__init__(context_size, dim_in, n_heads=1, dropout=0.6, qkv_bias=False)</code>","text":"<p>Initialise module.</p> <p>Parameters:</p> Name Type Description Default <code>dim_in</code> <code>int</code> <p>Dimension of input word embeddings.</p> required <code>context_size</code> <code>int</code> <p>The number of input word embeddings in the sequence.</p> required <code>n_heads</code> <code>int</code> <p>The number of attention heads. Defaults to 1.</p> <code>1</code> <code>dropout</code> <code>float</code> <p>The dropout rate. Defaults to 0.6.</p> <code>0.6</code> <code>qkv_bias</code> <code>bool</code> <p>Whether or not to include bias in the linear layers used to compute W_query, W_key and W_value. Defaults to False.</p> <code>False</code> Source code in <code>src/llmz/components/transformers.py</code> <pre><code>def __init__(\n    self,\n    context_size: int,\n    dim_in: int,\n    n_heads: int = 1,\n    dropout: float = 0.6,\n    qkv_bias: bool = False,\n):\n    \"\"\"Initialise module.\n\n    Args:\n        dim_in: Dimension of input word embeddings.\n        context_size: The number of input word embeddings in the sequence.\n        n_heads: The number of attention heads. Defaults to 1.\n        dropout: The dropout rate. Defaults to 0.6.\n        qkv_bias: Whether or not to include bias in the linear layers used to\n            compute W_query, W_key and W_value. Defaults to False.\n\n    \"\"\"\n    super().__init__()\n    self.attention = MultiHeadAttention(\n        context_size, dim_in, dim_in, n_heads, dropout, qkv_bias\n    )\n    self.linear_1 = nn.Linear(dim_in, dim_in * 2)\n    self.linear_2 = nn.Linear(dim_in * 2, dim_in)\n    self.normalise_1 = LayerNormalisation(dim_in)\n    self.normalise_2 = LayerNormalisation(dim_in)\n    self.dropout = nn.Dropout(dropout)\n    self.gelu = GELU()\n</code></pre>"},{"location":"api/#llmz.components.transformers.TransformerBlockGPT2.forward","title":"<code>forward(x)</code>","text":"<p>Execute the module's forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Batch of token embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Batch of attention weighted embeddings.</p> Source code in <code>src/llmz/components/transformers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Execute the module's forward pass.\n\n    Args:\n        x: Batch of token embeddings.\n\n    Returns:\n        Batch of attention weighted embeddings.\n\n    \"\"\"\n    y1 = self.normalise_1(x)\n    y1 = self.attention(y1)\n    y1 = self.dropout(y1)\n\n    y2 = self.normalise_2(y1 + x)\n    y2 = self.linear_1(y2)\n    y2 = self.gelu(y2)\n    y2 = self.linear_2(y2)\n    y2 = self.dropout(y2)\n\n    return y1 + y2\n</code></pre>"},{"location":"api/#llmz.datasets.GPTSmallTextDataset","title":"<code>GPTSmallTextDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>GPT dataset interface for any 'small' text data.</p> <p>This will tokenize all text in-memory using a GPT2's tokenization algorithm, which is a pre-trained Bite Pair Encoding (BPE).</p> Source code in <code>src/llmz/datasets.py</code> <pre><code>class GPTSmallTextDataset(Dataset):\n    \"\"\"GPT dataset interface for any 'small' text data.\n\n    This will tokenize all text in-memory using a GPT2's tokenization algorithm, which\n    is a pre-trained Bite Pair Encoding (BPE).\n    \"\"\"\n\n    def __init__(self, text: str, max_length: int = 256, stride: int = 128):\n        \"\"\"Initialise.\n\n        Args:\n            text: Raw text data to convert into tokens.\n            max_length: Number of tokens for each data instance. Defaults to 256.\n            stride: Separation (in tokens) between consecutive instances. Defaults to\n                128.\n\n        \"\"\"\n        tokenizer = tiktoken.get_encoding(\"gpt2\")\n        tokens = tokenizer.encode(text)\n\n        n_tokens = len(tokens)\n        n_instances = int((n_tokens - max_length) / stride)\n        if n_instances == 0:\n            raise RuntimeError(\"max_length + stride &lt;= number of tokens\")\n\n        self._X = torch.ones((n_instances, max_length))\n        self._y = torch.ones((n_instances, max_length))\n\n        for n, i in enumerate(range(0, n_tokens - max_length, stride)):\n            self._X[n,] = torch.tensor(tokens[i : i + max_length])\n            self._y[n,] = torch.tensor(tokens[i + 1 : i + max_length + 1])\n\n    def create_data_loader(\n        self,\n        batch_size: int = 4,\n        shuffle: bool = True,\n        drop_last: bool = True,\n        num_workers: int = 0,\n    ) -&gt; DataLoader:\n        \"\"\"Create data loader.\n\n        Args:\n            batch_size: The batch size. Defaults to 4.\n            shuffle: Whether to randomise instance order after each iteration. Defaults\n                to True.\n            drop_last: Drop last batch if less than `batch_size`. Defaults to True.\n            num_workers: Number of CPU processes to use for pre-processing. Defaults to\n                0.\n\n        Returns:\n            A fully configured DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=drop_last,\n            num_workers=num_workers,\n        )\n\n    def __len__(self) -&gt; int:\n        return self._X.size(0)\n\n    def __getitem__(self, idx: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        return self._X[idx,], self._y[idx,]\n</code></pre>"},{"location":"api/#llmz.datasets.GPTSmallTextDataset.__init__","title":"<code>__init__(text, max_length=256, stride=128)</code>","text":"<p>Initialise.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Raw text data to convert into tokens.</p> required <code>max_length</code> <code>int</code> <p>Number of tokens for each data instance. Defaults to 256.</p> <code>256</code> <code>stride</code> <code>int</code> <p>Separation (in tokens) between consecutive instances. Defaults to 128.</p> <code>128</code> Source code in <code>src/llmz/datasets.py</code> <pre><code>def __init__(self, text: str, max_length: int = 256, stride: int = 128):\n    \"\"\"Initialise.\n\n    Args:\n        text: Raw text data to convert into tokens.\n        max_length: Number of tokens for each data instance. Defaults to 256.\n        stride: Separation (in tokens) between consecutive instances. Defaults to\n            128.\n\n    \"\"\"\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n    tokens = tokenizer.encode(text)\n\n    n_tokens = len(tokens)\n    n_instances = int((n_tokens - max_length) / stride)\n    if n_instances == 0:\n        raise RuntimeError(\"max_length + stride &lt;= number of tokens\")\n\n    self._X = torch.ones((n_instances, max_length))\n    self._y = torch.ones((n_instances, max_length))\n\n    for n, i in enumerate(range(0, n_tokens - max_length, stride)):\n        self._X[n,] = torch.tensor(tokens[i : i + max_length])\n        self._y[n,] = torch.tensor(tokens[i + 1 : i + max_length + 1])\n</code></pre>"},{"location":"api/#llmz.datasets.GPTSmallTextDataset.create_data_loader","title":"<code>create_data_loader(batch_size=4, shuffle=True, drop_last=True, num_workers=0)</code>","text":"<p>Create data loader.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size. Defaults to 4.</p> <code>4</code> <code>shuffle</code> <code>bool</code> <p>Whether to randomise instance order after each iteration. Defaults to True.</p> <code>True</code> <code>drop_last</code> <code>bool</code> <p>Drop last batch if less than <code>batch_size</code>. Defaults to True.</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>Number of CPU processes to use for pre-processing. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>A fully configured DataLoader</p> Source code in <code>src/llmz/datasets.py</code> <pre><code>def create_data_loader(\n    self,\n    batch_size: int = 4,\n    shuffle: bool = True,\n    drop_last: bool = True,\n    num_workers: int = 0,\n) -&gt; DataLoader:\n    \"\"\"Create data loader.\n\n    Args:\n        batch_size: The batch size. Defaults to 4.\n        shuffle: Whether to randomise instance order after each iteration. Defaults\n            to True.\n        drop_last: Drop last batch if less than `batch_size`. Defaults to True.\n        num_workers: Number of CPU processes to use for pre-processing. Defaults to\n            0.\n\n    Returns:\n        A fully configured DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        drop_last=drop_last,\n        num_workers=num_workers,\n    )\n</code></pre>"},{"location":"api/#llmz.evaluate.EvalResult","title":"<code>EvalResult</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Container for evaluation results produced during training.</p> Source code in <code>src/llmz/evaluate.py</code> <pre><code>class EvalResult(NamedTuple):\n    \"\"\"Container for evaluation results produced during training.\"\"\"\n\n    step: int\n    results: dict[str, Result]\n</code></pre>"},{"location":"api/#llmz.evaluate.Evaluator","title":"<code>Evaluator</code>","text":"<p>Model evaluator.</p> <p>This class executes and stores all model evaluations during training.</p> Source code in <code>src/llmz/evaluate.py</code> <pre><code>class Evaluator:\n    \"\"\"Model evaluator.\n\n    This class executes and stores all model evaluations during training.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_dataloader: DataLoader,\n        val_dataloader: DataLoader,\n        metrics_fn: Callable[[nn.Module, DataLoader], dict[str, Result]],\n        scenarios_fn: Callable[[nn.Module], dict[str, Result]] | None = None,\n    ):\n        \"\"\"Initialise.\n\n        Args:\n            train_dataloader: DataLoader for training data.\n            val_dataloader: DataLoader for validation data.\n            metrics_fn: Callable that returns a dictionary of metrics given a model and\n                a dataloader.\n            scenarios_fn: Optional callable that returns a dictionary of results/outputs\n                given a model - e.g., generated text given an example prompt. Defaults\n                to None.\n\n        \"\"\"\n        self.train_dl = train_dataloader\n        self.val_dl = val_dataloader\n        self.metrics_fn = metrics_fn\n        self.scenarios_fn = scenarios_fn\n        self._eval_records: list[EvalResult] = []\n\n    def evaluate(\n        self, step: int, model: nn.Module, log: logging.Logger | None = None\n    ) -&gt; None:\n        \"\"\"Evaluate model.\n\n        Args:\n            step: The number of training steps applied to the model.\n            model: The model to evaluate.\n            log: Optional logger for logging results? Defaults to custom llmz logger.\n\n        Return:\n            All evaluations for the model after training steps.\n\n        \"\"\"\n        train_metrics = {\n            f\"train_{k}\": v for k, v in self.metrics_fn(model, self.train_dl).items()\n        }\n        val_metrics = {\n            f\"val_{k}\": v for k, v in self.metrics_fn(model, self.val_dl).items()\n        }\n        scenarios = self.scenarios_fn(model) if self.scenarios_fn else {}\n\n        eval_record = EvalResult(step, {**train_metrics, **val_metrics, **scenarios})\n        self._eval_records.append(eval_record)\n\n        if log:\n            log_msg = f\"{eval_record.step=}: \" + \", \".join(\n                f\"{k}={v}\" for k, v in eval_record.results.items()\n            )\n            log.info(log_msg)\n\n    def __getitem__(self, idx: int) -&gt; EvalResult:\n        return self._eval_records[idx]\n\n    def __iter__(self) -&gt; Iterator[EvalResult]:\n        return iter(self._eval_records)\n\n    def __len__(self) -&gt; int:\n        return len(self._eval_records)\n</code></pre>"},{"location":"api/#llmz.evaluate.Evaluator.__init__","title":"<code>__init__(train_dataloader, val_dataloader, metrics_fn, scenarios_fn=None)</code>","text":"<p>Initialise.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataloader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_dataloader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> required <code>metrics_fn</code> <code>Callable[[Module, DataLoader], dict[str, Result]]</code> <p>Callable that returns a dictionary of metrics given a model and a dataloader.</p> required <code>scenarios_fn</code> <code>Callable[[Module], dict[str, Result]] | None</code> <p>Optional callable that returns a dictionary of results/outputs given a model - e.g., generated text given an example prompt. Defaults to None.</p> <code>None</code> Source code in <code>src/llmz/evaluate.py</code> <pre><code>def __init__(\n    self,\n    train_dataloader: DataLoader,\n    val_dataloader: DataLoader,\n    metrics_fn: Callable[[nn.Module, DataLoader], dict[str, Result]],\n    scenarios_fn: Callable[[nn.Module], dict[str, Result]] | None = None,\n):\n    \"\"\"Initialise.\n\n    Args:\n        train_dataloader: DataLoader for training data.\n        val_dataloader: DataLoader for validation data.\n        metrics_fn: Callable that returns a dictionary of metrics given a model and\n            a dataloader.\n        scenarios_fn: Optional callable that returns a dictionary of results/outputs\n            given a model - e.g., generated text given an example prompt. Defaults\n            to None.\n\n    \"\"\"\n    self.train_dl = train_dataloader\n    self.val_dl = val_dataloader\n    self.metrics_fn = metrics_fn\n    self.scenarios_fn = scenarios_fn\n    self._eval_records: list[EvalResult] = []\n</code></pre>"},{"location":"api/#llmz.evaluate.Evaluator.evaluate","title":"<code>evaluate(step, model, log=None)</code>","text":"<p>Evaluate model.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The number of training steps applied to the model.</p> required <code>model</code> <code>Module</code> <p>The model to evaluate.</p> required <code>log</code> <code>Logger | None</code> <p>Optional logger for logging results? Defaults to custom llmz logger.</p> <code>None</code> Return <p>All evaluations for the model after training steps.</p> Source code in <code>src/llmz/evaluate.py</code> <pre><code>def evaluate(\n    self, step: int, model: nn.Module, log: logging.Logger | None = None\n) -&gt; None:\n    \"\"\"Evaluate model.\n\n    Args:\n        step: The number of training steps applied to the model.\n        model: The model to evaluate.\n        log: Optional logger for logging results? Defaults to custom llmz logger.\n\n    Return:\n        All evaluations for the model after training steps.\n\n    \"\"\"\n    train_metrics = {\n        f\"train_{k}\": v for k, v in self.metrics_fn(model, self.train_dl).items()\n    }\n    val_metrics = {\n        f\"val_{k}\": v for k, v in self.metrics_fn(model, self.val_dl).items()\n    }\n    scenarios = self.scenarios_fn(model) if self.scenarios_fn else {}\n\n    eval_record = EvalResult(step, {**train_metrics, **val_metrics, **scenarios})\n    self._eval_records.append(eval_record)\n\n    if log:\n        log_msg = f\"{eval_record.step=}: \" + \", \".join(\n            f\"{k}={v}\" for k, v in eval_record.results.items()\n        )\n        log.info(log_msg)\n</code></pre>"},{"location":"api/#llmz.evaluate.basic_llm_metrics","title":"<code>basic_llm_metrics(model, dl)</code>","text":"<p>Compute basic LLM metrics for a dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to use for inference.</p> required <code>dl</code> <code>DataLoader</code> <p>Dataloader with data batches for inference.</p> required Source code in <code>src/llmz/evaluate.py</code> <pre><code>def basic_llm_metrics(model: nn.Module, dl: DataLoader) -&gt; dict[str, float]:\n    \"\"\"Compute basic LLM metrics for a dataloader.\n\n    Args:\n        model: Model to use for inference.\n        dl: Dataloader with data batches for inference.\n\n    \"\"\"\n    loss = sum(\n        f.cross_entropy(model(X).flatten(0, 1), y.flatten()).item() for X, y in dl\n    ) / len(dl)\n    return {\"loss\": loss}\n</code></pre>"},{"location":"api/#llmz.generate.decode","title":"<code>decode(token_logits, strategy='greedy', temperature=1.0, *, k=5)</code>","text":"<p>Decode generative model output using the specified strategy.</p> Source code in <code>src/llmz/generate.py</code> <pre><code>def decode(\n    token_logits: torch.Tensor,\n    strategy: Literal[\"greedy\", \"sample\", \"topk\"] = \"greedy\",\n    temperature: float = 1.0,\n    *,\n    k: int = 5,\n) -&gt; int:\n    \"\"\"Decode generative model output using the specified strategy.\"\"\"\n    match strategy:\n        case \"greedy\":\n            return _greedy_decoding(token_logits, temperature)\n        case \"topk\":\n            return _top_k_decoding(token_logits, temperature, k)\n        case \"sample\":\n            return _sample_decoding(token_logits, temperature)\n</code></pre>"},{"location":"api/#llmz.generate.format_generated_words","title":"<code>format_generated_words(text, prompt)</code>","text":"<p>Format list of words into a readable paragraph.</p> Source code in <code>src/llmz/generate.py</code> <pre><code>def format_generated_words(text: str, prompt: str) -&gt; str:\n    \"\"\"Format list of words into a readable paragraph.\"\"\"\n    text = _capitalise_sentences(text, sentence_delimiter=\". \")\n    text = \"==&gt; \" + prompt.upper().strip() + \" \" + text.strip()\n    return \"\\n\".join([line for line in wrap(text, width=89)])\n</code></pre>"},{"location":"api/#llmz.generate.generate","title":"<code>generate(model, prompt, tokenizer, strategy='greedy', output_length=60, temperature=1.0, random_seed=42, device=torch.device('cpu'), *, k=2)</code>","text":"<p>Generate new text conditional on a text prompt.</p> Source code in <code>src/llmz/generate.py</code> <pre><code>def generate(\n    model: nn.Module,\n    prompt: str,\n    tokenizer: _Tokenizer,\n    strategy: Literal[\"greedy\", \"sample\", \"topk\"] = \"greedy\",\n    output_length: int = 60,\n    temperature: float = 1.0,\n    random_seed: int = 42,\n    device: torch.device = torch.device(\"cpu\"),\n    *,\n    k: int = 2,\n) -&gt; str:\n    \"\"\"Generate new text conditional on a text prompt.\"\"\"\n    torch.manual_seed(random_seed)\n\n    model.to(device)\n    model.eval()\n\n    prompt_tokens = tokenizer(prompt)\n    token_sequence = prompt_tokens.copy()\n    for _ in range(output_length):\n        x = torch.tensor([token_sequence], device=device)\n        token_logits = model(x)\n        token_pred = decode(token_logits[0, -1], strategy, temperature, k=k)\n        token_sequence += [token_pred]\n\n    new_token_sequence = token_sequence[len(prompt_tokens) :]\n    new_token_sequence = token_sequence[len(prompt_tokens) :]\n    return format_generated_words(tokenizer.tokens2text(new_token_sequence), prompt)\n</code></pre>"},{"location":"api/#llmz.generate.print_wrapped","title":"<code>print_wrapped(text, width=89)</code>","text":"<p>Print text with word wrapping.</p> Source code in <code>src/llmz/generate.py</code> <pre><code>def print_wrapped(text: str, width: int = 89) -&gt; None:\n    \"\"\"Print text with word wrapping.\"\"\"\n    wrapped_text = \"\\n\".join(wrap(text, width=width))\n    print(wrapped_text)\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2","title":"<code>GPT2</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implementation of OpenAI's GPT2 model.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>class GPT2(nn.Module):\n    \"\"\"Implementation of OpenAI's GPT2 model.\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        embed_dim: int,\n        context_size: int,\n        n_tsfmr_blocks: int = 1,\n        n_attn_heads: int = 1,\n        dropout: float = 0.6,\n        qkv_bias: bool = False,\n    ):\n        \"\"\"Initialise model.\n\n        Args:\n            vocab_size: The number of unique tokens that the model expects to encounter.\n            embed_dim: Dimension of input word embeddings.\n            context_size: The number of input word embeddings in the sequence.\n            n_tsfmr_blocks: The number of transformer blocks stacked together.\n            n_attn_heads: The number of attention heads in every transformer block.\n                Defaults to 1.\n            dropout: The dropout rate. Defaults to 0.6.\n            qkv_bias: Whether or not to include bias in the linear layers used to\n                compute W_query, W_key and W_value. Defaults to False.\n\n        \"\"\"\n        super().__init__()\n\n        self.context_size = context_size\n        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n        self.position_embed = nn.Embedding(context_size, embed_dim)\n        self.dropout_embed = nn.Dropout(p=dropout)\n\n        self.tsfmr_stack = nn.Sequential(\n            *[\n                TransformerBlockGPT2(\n                    context_size, embed_dim, n_attn_heads, dropout, qkv_bias\n                )\n                for _ in range(n_tsfmr_blocks)\n            ]\n        )\n\n        self.final_norm = LayerNormalisation(embed_dim)\n        self.output_head = nn.Linear(embed_dim, vocab_size, bias=False)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Execute the module's forward pass.\n\n        Args:\n            x: Batch of token embeddings.\n\n        Returns:\n            Batch of attention weighted embeddings.\n\n        \"\"\"\n        seq_len = x.size()[1]\n        if seq_len &gt; self.context_size:\n            msg = f\"seq_len ({seq_len}) &gt; context_size ({self.context_size})\"\n            raise GPT2InferenceError(msg)\n\n        positions = torch.arange(0, seq_len, device=x.device)\n        y = self.token_embed(x) + self.position_embed(positions)\n        y = self.dropout_embed(y)\n        y = self.tsfmr_stack(y)\n        y = self.final_norm(y)\n        logits = self.output_head(y)\n        return logits\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2.__init__","title":"<code>__init__(vocab_size, embed_dim, context_size, n_tsfmr_blocks=1, n_attn_heads=1, dropout=0.6, qkv_bias=False)</code>","text":"<p>Initialise model.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>The number of unique tokens that the model expects to encounter.</p> required <code>embed_dim</code> <code>int</code> <p>Dimension of input word embeddings.</p> required <code>context_size</code> <code>int</code> <p>The number of input word embeddings in the sequence.</p> required <code>n_tsfmr_blocks</code> <code>int</code> <p>The number of transformer blocks stacked together.</p> <code>1</code> <code>n_attn_heads</code> <code>int</code> <p>The number of attention heads in every transformer block. Defaults to 1.</p> <code>1</code> <code>dropout</code> <code>float</code> <p>The dropout rate. Defaults to 0.6.</p> <code>0.6</code> <code>qkv_bias</code> <code>bool</code> <p>Whether or not to include bias in the linear layers used to compute W_query, W_key and W_value. Defaults to False.</p> <code>False</code> Source code in <code>src/llmz/gpt2.py</code> <pre><code>def __init__(\n    self,\n    vocab_size: int,\n    embed_dim: int,\n    context_size: int,\n    n_tsfmr_blocks: int = 1,\n    n_attn_heads: int = 1,\n    dropout: float = 0.6,\n    qkv_bias: bool = False,\n):\n    \"\"\"Initialise model.\n\n    Args:\n        vocab_size: The number of unique tokens that the model expects to encounter.\n        embed_dim: Dimension of input word embeddings.\n        context_size: The number of input word embeddings in the sequence.\n        n_tsfmr_blocks: The number of transformer blocks stacked together.\n        n_attn_heads: The number of attention heads in every transformer block.\n            Defaults to 1.\n        dropout: The dropout rate. Defaults to 0.6.\n        qkv_bias: Whether or not to include bias in the linear layers used to\n            compute W_query, W_key and W_value. Defaults to False.\n\n    \"\"\"\n    super().__init__()\n\n    self.context_size = context_size\n    self.token_embed = nn.Embedding(vocab_size, embed_dim)\n    self.position_embed = nn.Embedding(context_size, embed_dim)\n    self.dropout_embed = nn.Dropout(p=dropout)\n\n    self.tsfmr_stack = nn.Sequential(\n        *[\n            TransformerBlockGPT2(\n                context_size, embed_dim, n_attn_heads, dropout, qkv_bias\n            )\n            for _ in range(n_tsfmr_blocks)\n        ]\n    )\n\n    self.final_norm = LayerNormalisation(embed_dim)\n    self.output_head = nn.Linear(embed_dim, vocab_size, bias=False)\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2.forward","title":"<code>forward(x)</code>","text":"<p>Execute the module's forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Batch of token embeddings.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Batch of attention weighted embeddings.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Execute the module's forward pass.\n\n    Args:\n        x: Batch of token embeddings.\n\n    Returns:\n        Batch of attention weighted embeddings.\n\n    \"\"\"\n    seq_len = x.size()[1]\n    if seq_len &gt; self.context_size:\n        msg = f\"seq_len ({seq_len}) &gt; context_size ({self.context_size})\"\n        raise GPT2InferenceError(msg)\n\n    positions = torch.arange(0, seq_len, device=x.device)\n    y = self.token_embed(x) + self.position_embed(positions)\n    y = self.dropout_embed(y)\n    y = self.tsfmr_stack(y)\n    y = self.final_norm(y)\n    logits = self.output_head(y)\n    return logits\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2Config","title":"<code>GPT2Config</code>  <code>dataclass</code>","text":"<p>Container class for GPT2 model hyper-parameters.</p> <p>This class will validate parameters and then allow GPT2 objects to be created using keyword argument expansion - e.g,</p> <pre><code>config = GPT2Config(...)\nmodel = GPT2(**config)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>The number of unique tokens that the model expects to encounter.</p> required <code>embed_dim</code> <code>int</code> <p>Dimension of input word embeddings.</p> required <code>context_size</code> <code>int</code> <p>The number of input word embeddings in the sequence.</p> required <code>n_tsfmr_blocks</code> <code>int</code> <p>The number of transformer blocks stacked together.</p> <code>1</code> <code>n_attn_heads</code> <code>int</code> <p>The number of attention heads in every transformer block. Defaults to 1.</p> <code>1</code> <code>dropout</code> <code>float</code> <p>The dropout rate. Defaults to 0.6.</p> <code>0.6</code> <code>qkv_bias</code> <code>bool</code> <p>Whether or not to include bias in the linear layers used to compute W_query, W_key and W_value. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>GPT2ConfigError</code> <p>if any int or float parameter is &lt;= 0, or embed_dim % n_attn_heads != 0</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>@dataclass(frozen=True)\nclass GPT2Config:\n    \"\"\"Container class for GPT2 model hyper-parameters.\n\n    This class will validate parameters and then allow GPT2 objects to be created using\n    keyword argument expansion - e.g,\n\n    ```python\n    config = GPT2Config(...)\n    model = GPT2(**config)\n    ```\n\n    Args:\n        vocab_size: The number of unique tokens that the model expects to encounter.\n        embed_dim: Dimension of input word embeddings.\n        context_size: The number of input word embeddings in the sequence.\n        n_tsfmr_blocks: The number of transformer blocks stacked together.\n        n_attn_heads: The number of attention heads in every transformer block.\n            Defaults to 1.\n        dropout: The dropout rate. Defaults to 0.6.\n        qkv_bias: Whether or not to include bias in the linear layers used to\n            compute W_query, W_key and W_value. Defaults to False.\n\n    Raises:\n        GPT2ConfigError: if any int or float parameter is &lt;= 0, or\n            embed_dim % n_attn_heads != 0\n\n    \"\"\"\n\n    vocab_size: int\n    embed_dim: int\n    context_size: int\n    n_tsfmr_blocks: int = 1\n    n_attn_heads: int = 1\n    dropout: float = 0.6\n    qkv_bias: bool = False\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate fields after initialisation.\"\"\"\n        errors: list[str] = []\n\n        for field, value in self.__dict__.items():\n            if type(value) in (int, float) and value &lt;= 0:\n                errors.append(f\"{field} is not &gt; 0\")\n\n        if self.embed_dim % self.n_attn_heads != 0:\n            errors.append(\"embed_dim % n_attn_heads != 0\")\n\n        if errors:\n            msg = \"invalid GPT2 parameters: \" + \"\\n \".join(errors)\n            raise GPT2ConfigError(msg)\n\n    def keys(self) -&gt; KeysView[str]:\n        \"\"\"Get iterator of field keys.\n\n        Part of Mapping protocol required to enable keyword argument expansion using the\n        `**` operator.\n        \"\"\"\n        return asdict(self).keys()\n\n    def __getitem__(self, key: str) -&gt; GPT2ConfigValue:\n        \"\"\"Get config value via its field name.\n\n        Part of Mapping protocol required to enable keyword argument expansion using the\n        `**` operator.\n        \"\"\"\n        return asdict(self)[key]\n\n    def __str__(self) -&gt; str:\n        \"\"\"Format config as a string.\"\"\"\n        str_repr = \"GPT2Config(\"\n        for key, value in asdict(self).items():\n            str_repr += f\"{key}={value}, \"\n        str_repr = str_repr[: len(str_repr) - 2]  # remove final ', '\n        str_repr += \")\"\n        return str_repr\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Format config for the command line.\"\"\"\n        cli_repr = \"GPT2Config(\\n\"\n        for key, value in asdict(self).items():\n            cli_repr += f\"  {key}={value},\\n\"\n        cli_repr += \")\"\n        return cli_repr\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2Config.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get config value via its field name.</p> <p>Part of Mapping protocol required to enable keyword argument expansion using the <code>**</code> operator.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>def __getitem__(self, key: str) -&gt; GPT2ConfigValue:\n    \"\"\"Get config value via its field name.\n\n    Part of Mapping protocol required to enable keyword argument expansion using the\n    `**` operator.\n    \"\"\"\n    return asdict(self)[key]\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2Config.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate fields after initialisation.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate fields after initialisation.\"\"\"\n    errors: list[str] = []\n\n    for field, value in self.__dict__.items():\n        if type(value) in (int, float) and value &lt;= 0:\n            errors.append(f\"{field} is not &gt; 0\")\n\n    if self.embed_dim % self.n_attn_heads != 0:\n        errors.append(\"embed_dim % n_attn_heads != 0\")\n\n    if errors:\n        msg = \"invalid GPT2 parameters: \" + \"\\n \".join(errors)\n        raise GPT2ConfigError(msg)\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2Config.__repr__","title":"<code>__repr__()</code>","text":"<p>Format config for the command line.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Format config for the command line.\"\"\"\n    cli_repr = \"GPT2Config(\\n\"\n    for key, value in asdict(self).items():\n        cli_repr += f\"  {key}={value},\\n\"\n    cli_repr += \")\"\n    return cli_repr\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2Config.__str__","title":"<code>__str__()</code>","text":"<p>Format config as a string.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Format config as a string.\"\"\"\n    str_repr = \"GPT2Config(\"\n    for key, value in asdict(self).items():\n        str_repr += f\"{key}={value}, \"\n    str_repr = str_repr[: len(str_repr) - 2]  # remove final ', '\n    str_repr += \")\"\n    return str_repr\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2Config.keys","title":"<code>keys()</code>","text":"<p>Get iterator of field keys.</p> <p>Part of Mapping protocol required to enable keyword argument expansion using the <code>**</code> operator.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>def keys(self) -&gt; KeysView[str]:\n    \"\"\"Get iterator of field keys.\n\n    Part of Mapping protocol required to enable keyword argument expansion using the\n    `**` operator.\n    \"\"\"\n    return asdict(self).keys()\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2ConfigError","title":"<code>GPT2ConfigError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for GPT2 inference errors.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>class GPT2ConfigError(Exception):\n    \"\"\"Custom exception for GPT2 inference errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2InferenceError","title":"<code>GPT2InferenceError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for GPT2 inference errors.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>class GPT2InferenceError(Exception):\n    \"\"\"Custom exception for GPT2 inference errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2Tokenizer","title":"<code>GPT2Tokenizer</code>","text":"<p>               Bases: <code>_Tokenizer</code></p> <p>Pre-trained version of GPT2's tokenizer.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>class GPT2Tokenizer(_Tokenizer):\n    \"\"\"Pre-trained version of GPT2's tokenizer.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialise tokenizer.\"\"\"\n        self._tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    def text2tokens(self, text: str) -&gt; list[int]:\n        \"\"\"Map a string to a list of tokens.\"\"\"\n        return self._tokenizer.encode(text)\n\n    def tokens2text(self, tokens: list[int]) -&gt; str:\n        \"\"\"Map a list of tokens to a string..\"\"\"\n        return self._tokenizer.decode(tokens)\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2Tokenizer.__init__","title":"<code>__init__()</code>","text":"<p>Initialise tokenizer.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialise tokenizer.\"\"\"\n    self._tokenizer = tiktoken.get_encoding(\"gpt2\")\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2Tokenizer.text2tokens","title":"<code>text2tokens(text)</code>","text":"<p>Map a string to a list of tokens.</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>def text2tokens(self, text: str) -&gt; list[int]:\n    \"\"\"Map a string to a list of tokens.\"\"\"\n    return self._tokenizer.encode(text)\n</code></pre>"},{"location":"api/#llmz.gpt2.GPT2Tokenizer.tokens2text","title":"<code>tokens2text(tokens)</code>","text":"<p>Map a list of tokens to a string..</p> Source code in <code>src/llmz/gpt2.py</code> <pre><code>def tokens2text(self, tokens: list[int]) -&gt; str:\n    \"\"\"Map a list of tokens to a string..\"\"\"\n    return self._tokenizer.decode(tokens)\n</code></pre>"},{"location":"api/#llmz.train.GradientClipCallback","title":"<code>GradientClipCallback</code>","text":"<p>Callable class that clips model gradient using max norm.</p> Source code in <code>src/llmz/train.py</code> <pre><code>class GradientClipCallback:\n    \"\"\"Callable class that clips model gradient using max norm.\"\"\"\n\n    def __init__(self, clip_grad_norm: float = torch.inf):\n        \"\"\"Initialise.\"\"\"\n        self.clip_grad_norm = clip_grad_norm\n\n    def __call__(self, model: nn.Module) -&gt; None:\n        \"\"\"Clip model gradients.\"\"\"\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=self.clip_grad_norm)\n</code></pre>"},{"location":"api/#llmz.train.GradientClipCallback.__call__","title":"<code>__call__(model)</code>","text":"<p>Clip model gradients.</p> Source code in <code>src/llmz/train.py</code> <pre><code>def __call__(self, model: nn.Module) -&gt; None:\n    \"\"\"Clip model gradients.\"\"\"\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=self.clip_grad_norm)\n</code></pre>"},{"location":"api/#llmz.train.GradientClipCallback.__init__","title":"<code>__init__(clip_grad_norm=torch.inf)</code>","text":"<p>Initialise.</p> Source code in <code>src/llmz/train.py</code> <pre><code>def __init__(self, clip_grad_norm: float = torch.inf):\n    \"\"\"Initialise.\"\"\"\n    self.clip_grad_norm = clip_grad_norm\n</code></pre>"},{"location":"api/#llmz.train.LinearWarmupCosineAnnealingLRSchedule","title":"<code>LinearWarmupCosineAnnealingLRSchedule</code>","text":"<p>LR schedule using cosine annealing with linear warmup.</p> Source code in <code>src/llmz/train.py</code> <pre><code>class LinearWarmupCosineAnnealingLRSchedule:\n    \"\"\"LR schedule using cosine annealing with linear warmup.\"\"\"\n\n    def __init__(\n        self, num_steps: int, warmup_steps: int, initial_lr: float, peak_lr: float\n    ):\n        \"\"\"Initialise.\n\n        Args:\n            num_steps: The total number of steps for the schedule.\n            warmup_steps: Number of steps in the linear warmup phase.\n            initial_lr: Learning rate at first step.\n            peak_lr: Peak learning rate at end of warmup phase.\n\n        \"\"\"\n        value_errors: list[str] = []\n        if num_steps &lt;= 0:\n            value_errors.append(\" * num_steps &lt;= 0\")\n        if warmup_steps &gt; num_steps:\n            value_errors.append(\" * warmup_steps &gt; num_steps\")\n        if initial_lr &lt;= 0.0:\n            value_errors.append(\" * initial_lr &lt;= 0.0\")\n        if peak_lr &lt; initial_lr:\n            value_errors.append(\" * peak_lr &lt; initial_lr\")\n\n        if value_errors:\n            e = ValueError(\"Invalid arguments for LR schedule\")\n            for error in value_errors:\n                e.add_note(error)\n            raise e\n\n        self.num_steps = num_steps\n        self.warmup_steps = warmup_steps\n        self.cosine_steps = num_steps - warmup_steps\n        self.initial_lr = initial_lr\n        self.lr_cosine_delta = peak_lr - initial_lr\n        self.lr_warmup_delta = (peak_lr - initial_lr) / warmup_steps\n\n    def __call__(self, step: int) -&gt; float:\n        \"\"\"Get learning rate for given step.\n\n        Args:\n            step: The global training step.\n\n        Returns:\n            The learning rate for the global training step.\n\n        Raises:\n            ValueError: If step &lt; 0.\n\n        \"\"\"\n        if step &lt; 0:\n            raise ValueError(f\"{step=}, must be &gt; 0\")\n        elif step &gt;= 0 and step &lt; self.warmup_steps:\n            lr = self.initial_lr + step * self.lr_warmup_delta\n        elif step &gt;= self.warmup_steps and step &lt;= self.num_steps:\n            step_cosine = step - self.warmup_steps\n            x = math.pi * step_cosine / self.cosine_steps\n            lr = self.initial_lr + self.lr_cosine_delta * 0.5 * (1.0 + math.cos(x))\n        else:\n            lr = self.initial_lr\n\n        return lr\n</code></pre>"},{"location":"api/#llmz.train.LinearWarmupCosineAnnealingLRSchedule.__call__","title":"<code>__call__(step)</code>","text":"<p>Get learning rate for given step.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The global training step.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The learning rate for the global training step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If step &lt; 0.</p> Source code in <code>src/llmz/train.py</code> <pre><code>def __call__(self, step: int) -&gt; float:\n    \"\"\"Get learning rate for given step.\n\n    Args:\n        step: The global training step.\n\n    Returns:\n        The learning rate for the global training step.\n\n    Raises:\n        ValueError: If step &lt; 0.\n\n    \"\"\"\n    if step &lt; 0:\n        raise ValueError(f\"{step=}, must be &gt; 0\")\n    elif step &gt;= 0 and step &lt; self.warmup_steps:\n        lr = self.initial_lr + step * self.lr_warmup_delta\n    elif step &gt;= self.warmup_steps and step &lt;= self.num_steps:\n        step_cosine = step - self.warmup_steps\n        x = math.pi * step_cosine / self.cosine_steps\n        lr = self.initial_lr + self.lr_cosine_delta * 0.5 * (1.0 + math.cos(x))\n    else:\n        lr = self.initial_lr\n\n    return lr\n</code></pre>"},{"location":"api/#llmz.train.LinearWarmupCosineAnnealingLRSchedule.__init__","title":"<code>__init__(num_steps, warmup_steps, initial_lr, peak_lr)</code>","text":"<p>Initialise.</p> <p>Parameters:</p> Name Type Description Default <code>num_steps</code> <code>int</code> <p>The total number of steps for the schedule.</p> required <code>warmup_steps</code> <code>int</code> <p>Number of steps in the linear warmup phase.</p> required <code>initial_lr</code> <code>float</code> <p>Learning rate at first step.</p> required <code>peak_lr</code> <code>float</code> <p>Peak learning rate at end of warmup phase.</p> required Source code in <code>src/llmz/train.py</code> <pre><code>def __init__(\n    self, num_steps: int, warmup_steps: int, initial_lr: float, peak_lr: float\n):\n    \"\"\"Initialise.\n\n    Args:\n        num_steps: The total number of steps for the schedule.\n        warmup_steps: Number of steps in the linear warmup phase.\n        initial_lr: Learning rate at first step.\n        peak_lr: Peak learning rate at end of warmup phase.\n\n    \"\"\"\n    value_errors: list[str] = []\n    if num_steps &lt;= 0:\n        value_errors.append(\" * num_steps &lt;= 0\")\n    if warmup_steps &gt; num_steps:\n        value_errors.append(\" * warmup_steps &gt; num_steps\")\n    if initial_lr &lt;= 0.0:\n        value_errors.append(\" * initial_lr &lt;= 0.0\")\n    if peak_lr &lt; initial_lr:\n        value_errors.append(\" * peak_lr &lt; initial_lr\")\n\n    if value_errors:\n        e = ValueError(\"Invalid arguments for LR schedule\")\n        for error in value_errors:\n            e.add_note(error)\n        raise e\n\n    self.num_steps = num_steps\n    self.warmup_steps = warmup_steps\n    self.cosine_steps = num_steps - warmup_steps\n    self.initial_lr = initial_lr\n    self.lr_cosine_delta = peak_lr - initial_lr\n    self.lr_warmup_delta = (peak_lr - initial_lr) / warmup_steps\n</code></pre>"},{"location":"api/#llmz.train.autoregressive_llm_loss","title":"<code>autoregressive_llm_loss(model, X_batch, y_batch)</code>","text":"<p>Compute loss for AR LLMs like GPTs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The language model.</p> required <code>X_batch</code> <code>Tensor</code> <p>Batch of input tokens.</p> required <code>y_batch</code> <code>Tensor</code> <p>Batch of output tokens - i.e., next token from the input sequence.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Mean cross-entropy loss for the batch.</p> Source code in <code>src/llmz/train.py</code> <pre><code>def autoregressive_llm_loss(\n    model: nn.Module, X_batch: torch.Tensor, y_batch: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Compute loss for AR LLMs like GPTs.\n\n    Args:\n        model: The language model.\n        X_batch: Batch of input tokens.\n        y_batch: Batch of output tokens - i.e., next token from the input sequence.\n\n    Returns:\n        Mean cross-entropy loss for the batch.\n\n    \"\"\"\n    # model outputs logits as softmax is implemented in cross-entropy calc.\n    logits = model(X_batch)\n\n    # flatten logits from [BATCH, SEQ_LEN, N_CLASSES] to [BATCH * SEQ_LEN, N_CLASSES]\n    # flatten y_batch from [BATCH, SEQ_LEN] to [BATCH * SEQ_LEN]\n    loss = nn.functional.cross_entropy(logits.flatten(0, 1), y_batch.flatten())\n    return loss\n</code></pre>"},{"location":"api/#llmz.train.train","title":"<code>train(model, loss_calc, optimiser, lr_schedule, train_dataloader, train_epochs, eval_freq_steps, evaluator, model_backward_callbacks=None, log_freq_steps=100, device=torch.device('cpu'))</code>","text":"<p>Trains model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model to train.</p> required <code>loss_calc</code> <code>Callable[[Module, Tensor, Tensor], Tensor]</code> <p>Function that calculates and returns loss for model and batch.</p> required <code>optimiser</code> <code>Optimizer</code> <p>The optimizer for updating model parameters.</p> required <code>lr_schedule</code> <code>Callable[[int], float] | LRScheduler</code> <p>Function to compute learning rate for training step.</p> required <code>train_dataloader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>train_epochs</code> <code>int</code> <p>Number of training epochs.</p> required <code>eval_freq_steps</code> <code>int</code> <p>Number of steps between evaluations.</p> required <code>evaluator</code> <code>Evaluator</code> <p>A handler for all model evaluations.</p> required <code>model_backward_callbacks</code> <code>list[Callable[[Module], None]] | None</code> <p>Optional callbacks for model after backward pass.</p> <code>None</code> <code>log_freq_steps</code> <code>int</code> <p>Number of steps between basic progress logging to stdout. Defaults to 100.</p> <code>100</code> <code>device</code> <code>device</code> <p>The processor to use for training. Defaults to CPU.</p> <code>device('cpu')</code> Source code in <code>src/llmz/train.py</code> <pre><code>def train(\n    model: nn.Module,\n    loss_calc: Callable[[nn.Module, torch.Tensor, torch.Tensor], torch.Tensor],\n    optimiser: optim.Optimizer,\n    lr_schedule: Callable[[int], float] | optim.lr_scheduler.LRScheduler,\n    train_dataloader: DataLoader,\n    train_epochs: int,\n    eval_freq_steps: int,\n    evaluator: Evaluator,\n    model_backward_callbacks: list[Callable[[nn.Module], None]] | None = None,\n    log_freq_steps: int = 100,\n    device: torch.device = torch.device(\"cpu\"),\n) -&gt; None:\n    \"\"\"Trains model.\n\n    Args:\n        model: The PyTorch model to train.\n        loss_calc: Function that calculates and returns loss for model and batch.\n        optimiser: The optimizer for updating model parameters.\n        lr_schedule: Function to compute learning rate for training step.\n        train_dataloader: DataLoader for training data.\n        train_epochs: Number of training epochs.\n        eval_freq_steps: Number of steps between evaluations.\n        evaluator: A handler for all model evaluations.\n        model_backward_callbacks: Optional callbacks for model after backward pass.\n        log_freq_steps: Number of steps between basic progress logging to stdout.\n            Defaults to 100.\n        device: The processor to use for training. Defaults to CPU.\n\n    \"\"\"\n    if not isinstance(lr_schedule, optim.lr_scheduler.LRScheduler):\n        lr_schedule = optim.lr_scheduler.LambdaLR(optimiser, lr_schedule)\n\n    model = model.to(device)\n    step = 0\n\n    for epoch in range(1, train_epochs + 1):\n        for X_batch, y_batch in train_dataloader:\n            X_batch = X_batch.to(device, non_blocking=True)\n            y_batch = y_batch.to(device, non_blocking=True)\n\n            step += 1\n            model.train()\n            optimiser.zero_grad()\n\n            loss = loss_calc(model, X_batch, y_batch)\n            loss.backward()\n\n            if model_backward_callbacks:\n                for callback in model_backward_callbacks:\n                    callback(model)\n\n            optimiser.step()\n            lr_schedule.step()\n\n            if step % log_freq_steps == 0:\n                log.info(f\"{step=}, {epoch=}\")\n\n            if step % eval_freq_steps == 0:\n                evaluator.evaluate(step, model)\n</code></pre>"}]}