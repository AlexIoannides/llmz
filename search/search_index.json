{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLMz","text":"<p>These will be the docs for the llmz project.</p>"},{"location":"api/","title":"API Reference","text":"<p>Tokenizers for LLMs.</p>"},{"location":"api/#llmz.datasets.GPTSmallTextDataset","title":"<code>GPTSmallTextDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>GPT dataset interface for any 'small' text data.</p> <p>This will tokenize all text in-memory using a GPT2's tokenization algorithm, which is a pre-trained Bite Pair Encoding (BPE).</p> Source code in <code>src/llmz/datasets.py</code> <pre><code>class GPTSmallTextDataset(Dataset):\n    \"\"\"GPT dataset interface for any 'small' text data.\n\n    This will tokenize all text in-memory using a GPT2's tokenization algorithm, which\n    is a pre-trained Bite Pair Encoding (BPE).\n    \"\"\"\n\n    def __init__(self, text: str, max_length: int = 256, stride: int = 128):\n        \"\"\"Initialise.\n\n        Args:\n            text: Raw text data to convert into tokens.\n            max_length: Number of tokens for each data instance. Defaults to 256.\n            stride: Separation (in tokens) between consecutive instances. Defaults to\n                128.\n\n        \"\"\"\n        tokenizer = tiktoken.get_encoding(\"gpt2\")\n        tokens = tokenizer.encode(text)\n\n        n_tokens = len(tokens)\n        n_instances = int((n_tokens - max_length) / stride)\n        if n_instances == 0:\n            raise RuntimeError(\"max_length + stride &lt;= number of tokens\")\n\n        self._X = torch.ones((n_instances, max_length))\n        self._y = torch.ones((n_instances, max_length))\n\n        for n, i in enumerate(range(0, n_tokens - max_length, stride)):\n            self._X[n,] = torch.tensor(tokens[i : i + max_length])\n            self._y[n,] = torch.tensor(tokens[i + 1 : i + max_length + 1])\n\n    def create_data_loader(\n        self,\n        batch_size: int = 4,\n        shuffle: bool = True,\n        drop_last: bool = True,\n        num_workers: int = 0,\n    ) -&gt; DataLoader:\n        \"\"\"Create data loader.\n\n        Args:\n            batch_size: The batch size. Defaults to 4.\n            shuffle: Whether to randomise instance order after each iteration. Defaults\n                to True.\n            drop_last: Drop last batch if less than `batch_size`. Defaults to True.\n            num_workers: Number of CPU processes to use for pre-processing. Defaults to\n                0.\n\n        Returns:\n            A fully configured DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=drop_last,\n            num_workers=num_workers,\n        )\n\n    def __len__(self) -&gt; int:\n        return self._X.size(0)\n\n    def __getitem__(self, idx: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        return self._X[idx,], self._y[idx,]\n</code></pre>"},{"location":"api/#llmz.datasets.GPTSmallTextDataset.__init__","title":"<code>__init__(text, max_length=256, stride=128)</code>","text":"<p>Initialise.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Raw text data to convert into tokens.</p> required <code>max_length</code> <code>int</code> <p>Number of tokens for each data instance. Defaults to 256.</p> <code>256</code> <code>stride</code> <code>int</code> <p>Separation (in tokens) between consecutive instances. Defaults to 128.</p> <code>128</code> Source code in <code>src/llmz/datasets.py</code> <pre><code>def __init__(self, text: str, max_length: int = 256, stride: int = 128):\n    \"\"\"Initialise.\n\n    Args:\n        text: Raw text data to convert into tokens.\n        max_length: Number of tokens for each data instance. Defaults to 256.\n        stride: Separation (in tokens) between consecutive instances. Defaults to\n            128.\n\n    \"\"\"\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n    tokens = tokenizer.encode(text)\n\n    n_tokens = len(tokens)\n    n_instances = int((n_tokens - max_length) / stride)\n    if n_instances == 0:\n        raise RuntimeError(\"max_length + stride &lt;= number of tokens\")\n\n    self._X = torch.ones((n_instances, max_length))\n    self._y = torch.ones((n_instances, max_length))\n\n    for n, i in enumerate(range(0, n_tokens - max_length, stride)):\n        self._X[n,] = torch.tensor(tokens[i : i + max_length])\n        self._y[n,] = torch.tensor(tokens[i + 1 : i + max_length + 1])\n</code></pre>"},{"location":"api/#llmz.datasets.GPTSmallTextDataset.create_data_loader","title":"<code>create_data_loader(batch_size=4, shuffle=True, drop_last=True, num_workers=0)</code>","text":"<p>Create data loader.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size. Defaults to 4.</p> <code>4</code> <code>shuffle</code> <code>bool</code> <p>Whether to randomise instance order after each iteration. Defaults to True.</p> <code>True</code> <code>drop_last</code> <code>bool</code> <p>Drop last batch if less than <code>batch_size</code>. Defaults to True.</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>Number of CPU processes to use for pre-processing. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>A fully configured DataLoader</p> Source code in <code>src/llmz/datasets.py</code> <pre><code>def create_data_loader(\n    self,\n    batch_size: int = 4,\n    shuffle: bool = True,\n    drop_last: bool = True,\n    num_workers: int = 0,\n) -&gt; DataLoader:\n    \"\"\"Create data loader.\n\n    Args:\n        batch_size: The batch size. Defaults to 4.\n        shuffle: Whether to randomise instance order after each iteration. Defaults\n            to True.\n        drop_last: Drop last batch if less than `batch_size`. Defaults to True.\n        num_workers: Number of CPU processes to use for pre-processing. Defaults to\n            0.\n\n    Returns:\n        A fully configured DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        drop_last=drop_last,\n        num_workers=num_workers,\n    )\n</code></pre>"}]}